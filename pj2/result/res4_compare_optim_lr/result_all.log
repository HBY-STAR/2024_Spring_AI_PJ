cuda
Files already downloaded and verified
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]          36,864
       BatchNorm2d-4           [-1, 64, 32, 32]             128
           Dropout-5           [-1, 64, 32, 32]               0
            Conv2d-6           [-1, 64, 32, 32]          36,864
       BatchNorm2d-7           [-1, 64, 32, 32]             128
           Dropout-8           [-1, 64, 32, 32]               0
        BasicBlock-9           [-1, 64, 32, 32]               0
           Conv2d-10           [-1, 64, 32, 32]          36,864
      BatchNorm2d-11           [-1, 64, 32, 32]             128
          Dropout-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 64, 32, 32]          36,864
      BatchNorm2d-14           [-1, 64, 32, 32]             128
          Dropout-15           [-1, 64, 32, 32]               0
       BasicBlock-16           [-1, 64, 32, 32]               0
           Conv2d-17          [-1, 128, 16, 16]          73,728
      BatchNorm2d-18          [-1, 128, 16, 16]             256
          Dropout-19          [-1, 128, 16, 16]               0
           Conv2d-20          [-1, 128, 16, 16]         147,456
      BatchNorm2d-21          [-1, 128, 16, 16]             256
          Dropout-22          [-1, 128, 16, 16]               0
           Conv2d-23          [-1, 128, 16, 16]           8,192
      BatchNorm2d-24          [-1, 128, 16, 16]             256
          Dropout-25          [-1, 128, 16, 16]               0
       BasicBlock-26          [-1, 128, 16, 16]               0
           Conv2d-27          [-1, 128, 16, 16]         147,456
      BatchNorm2d-28          [-1, 128, 16, 16]             256
          Dropout-29          [-1, 128, 16, 16]               0
           Conv2d-30          [-1, 128, 16, 16]         147,456
      BatchNorm2d-31          [-1, 128, 16, 16]             256
          Dropout-32          [-1, 128, 16, 16]               0
       BasicBlock-33          [-1, 128, 16, 16]               0
           Conv2d-34            [-1, 256, 8, 8]         294,912
      BatchNorm2d-35            [-1, 256, 8, 8]             512
          Dropout-36            [-1, 256, 8, 8]               0
           Conv2d-37            [-1, 256, 8, 8]         589,824
      BatchNorm2d-38            [-1, 256, 8, 8]             512
          Dropout-39            [-1, 256, 8, 8]               0
           Conv2d-40            [-1, 256, 8, 8]          32,768
      BatchNorm2d-41            [-1, 256, 8, 8]             512
          Dropout-42            [-1, 256, 8, 8]               0
       BasicBlock-43            [-1, 256, 8, 8]               0
           Conv2d-44            [-1, 256, 8, 8]         589,824
      BatchNorm2d-45            [-1, 256, 8, 8]             512
          Dropout-46            [-1, 256, 8, 8]               0
           Conv2d-47            [-1, 256, 8, 8]         589,824
      BatchNorm2d-48            [-1, 256, 8, 8]             512
          Dropout-49            [-1, 256, 8, 8]               0
       BasicBlock-50            [-1, 256, 8, 8]               0
           Conv2d-51            [-1, 512, 4, 4]       1,179,648
      BatchNorm2d-52            [-1, 512, 4, 4]           1,024
          Dropout-53            [-1, 512, 4, 4]               0
           Conv2d-54            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-55            [-1, 512, 4, 4]           1,024
          Dropout-56            [-1, 512, 4, 4]               0
           Conv2d-57            [-1, 512, 4, 4]         131,072
      BatchNorm2d-58            [-1, 512, 4, 4]           1,024
          Dropout-59            [-1, 512, 4, 4]               0
       BasicBlock-60            [-1, 512, 4, 4]               0
           Conv2d-61            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-62            [-1, 512, 4, 4]           1,024
          Dropout-63            [-1, 512, 4, 4]               0
           Conv2d-64            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-65            [-1, 512, 4, 4]           1,024
          Dropout-66            [-1, 512, 4, 4]               0
       BasicBlock-67            [-1, 512, 4, 4]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,173,962
Trainable params: 11,173,962
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 15.44
Params size (MB): 42.63
Estimated Total Size (MB): 58.07
----------------------------------------------------------------
Running with Optimizer: SGD, Learning Rate: 0.005
EPOCHS : 0

Test set: Average loss: 1.6353, Accuracy: 4213/10000 (42.13%)

EPOCHS : 1

Test set: Average loss: 1.2560, Accuracy: 5467/10000 (54.67%)

EPOCHS : 2

Test set: Average loss: 1.1400, Accuracy: 6019/10000 (60.19%)

EPOCHS : 3

Test set: Average loss: 0.9764, Accuracy: 6619/10000 (66.19%)

EPOCHS : 4

Test set: Average loss: 0.9615, Accuracy: 6705/10000 (67.05%)

EPOCHS : 5

Test set: Average loss: 0.8444, Accuracy: 7122/10000 (71.22%)

EPOCHS : 6

Test set: Average loss: 0.8118, Accuracy: 7307/10000 (73.07%)

EPOCHS : 7

Test set: Average loss: 0.7068, Accuracy: 7616/10000 (76.16%)

EPOCHS : 8

Test set: Average loss: 0.6996, Accuracy: 7641/10000 (76.41%)

EPOCHS : 9

Test set: Average loss: 0.6217, Accuracy: 7906/10000 (79.06%)

EPOCHS : 10

Test set: Average loss: 0.6094, Accuracy: 7962/10000 (79.62%)

EPOCHS : 11

Test set: Average loss: 0.6242, Accuracy: 7943/10000 (79.43%)

EPOCHS : 12
Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.

Test set: Average loss: 0.6760, Accuracy: 7842/10000 (78.42%)

EPOCHS : 13

Test set: Average loss: 0.5045, Accuracy: 8343/10000 (83.43%)

EPOCHS : 14

Test set: Average loss: 0.5022, Accuracy: 8319/10000 (83.19%)

EPOCHS : 15

Test set: Average loss: 0.4962, Accuracy: 8365/10000 (83.65%)

EPOCHS : 16

Test set: Average loss: 0.4895, Accuracy: 8376/10000 (83.76%)

EPOCHS : 17

Test set: Average loss: 0.4892, Accuracy: 8374/10000 (83.74%)

EPOCHS : 18

Test set: Average loss: 0.4916, Accuracy: 8364/10000 (83.64%)

EPOCHS : 19
Epoch 00020: reducing learning rate of group 0 to 1.2500e-05.

Test set: Average loss: 0.4854, Accuracy: 8375/10000 (83.75%)

EPOCHS : 20

Test set: Average loss: 0.4847, Accuracy: 8388/10000 (83.88%)

EPOCHS : 21

Test set: Average loss: 0.4875, Accuracy: 8369/10000 (83.69%)

EPOCHS : 22
Epoch 00023: reducing learning rate of group 0 to 6.2500e-07.

Test set: Average loss: 0.4844, Accuracy: 8381/10000 (83.81%)

EPOCHS : 23

Test set: Average loss: 0.4848, Accuracy: 8373/10000 (83.73%)

EPOCHS : 24

Test set: Average loss: 0.4843, Accuracy: 8383/10000 (83.83%)

EPOCHS : 25

Test set: Average loss: 0.4849, Accuracy: 8382/10000 (83.82%)

EPOCHS : 26
Epoch 00027: reducing learning rate of group 0 to 3.1250e-08.

Test set: Average loss: 0.4852, Accuracy: 8379/10000 (83.79%)

EPOCHS : 27

Test set: Average loss: 0.4841, Accuracy: 8380/10000 (83.80%)

EPOCHS : 28

Test set: Average loss: 0.4865, Accuracy: 8368/10000 (83.68%)

EPOCHS : 29
Epoch 00030: reducing learning rate of group 0 to 1.5625e-09.

Test set: Average loss: 0.4836, Accuracy: 8382/10000 (83.82%)

EPOCHS : 30

Test set: Average loss: 0.4844, Accuracy: 8376/10000 (83.76%)

EPOCHS : 31

Test set: Average loss: 0.4859, Accuracy: 8373/10000 (83.73%)

EPOCHS : 32

Test set: Average loss: 0.4863, Accuracy: 8375/10000 (83.75%)

EPOCHS : 33

Test set: Average loss: 0.4856, Accuracy: 8372/10000 (83.72%)

EPOCHS : 34

Test set: Average loss: 0.4878, Accuracy: 8375/10000 (83.75%)

EPOCHS : 35

Test set: Average loss: 0.4829, Accuracy: 8383/10000 (83.83%)

EPOCHS : 36

Test set: Average loss: 0.4829, Accuracy: 8379/10000 (83.79%)

EPOCHS : 37

Test set: Average loss: 0.4851, Accuracy: 8376/10000 (83.76%)

EPOCHS : 38

Test set: Average loss: 0.4856, Accuracy: 8373/10000 (83.73%)

EPOCHS : 39

Test set: Average loss: 0.4834, Accuracy: 8374/10000 (83.74%)

Optimizer: SGD, Learning Rate: 0.005, Test Accuracy: 83.88
cuda
Files already downloaded and verified
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]          36,864
       BatchNorm2d-4           [-1, 64, 32, 32]             128
           Dropout-5           [-1, 64, 32, 32]               0
            Conv2d-6           [-1, 64, 32, 32]          36,864
       BatchNorm2d-7           [-1, 64, 32, 32]             128
           Dropout-8           [-1, 64, 32, 32]               0
        BasicBlock-9           [-1, 64, 32, 32]               0
           Conv2d-10           [-1, 64, 32, 32]          36,864
      BatchNorm2d-11           [-1, 64, 32, 32]             128
          Dropout-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 64, 32, 32]          36,864
      BatchNorm2d-14           [-1, 64, 32, 32]             128
          Dropout-15           [-1, 64, 32, 32]               0
       BasicBlock-16           [-1, 64, 32, 32]               0
           Conv2d-17          [-1, 128, 16, 16]          73,728
      BatchNorm2d-18          [-1, 128, 16, 16]             256
          Dropout-19          [-1, 128, 16, 16]               0
           Conv2d-20          [-1, 128, 16, 16]         147,456
      BatchNorm2d-21          [-1, 128, 16, 16]             256
          Dropout-22          [-1, 128, 16, 16]               0
           Conv2d-23          [-1, 128, 16, 16]           8,192
      BatchNorm2d-24          [-1, 128, 16, 16]             256
          Dropout-25          [-1, 128, 16, 16]               0
       BasicBlock-26          [-1, 128, 16, 16]               0
           Conv2d-27          [-1, 128, 16, 16]         147,456
      BatchNorm2d-28          [-1, 128, 16, 16]             256
          Dropout-29          [-1, 128, 16, 16]               0
           Conv2d-30          [-1, 128, 16, 16]         147,456
      BatchNorm2d-31          [-1, 128, 16, 16]             256
          Dropout-32          [-1, 128, 16, 16]               0
       BasicBlock-33          [-1, 128, 16, 16]               0
           Conv2d-34            [-1, 256, 8, 8]         294,912
      BatchNorm2d-35            [-1, 256, 8, 8]             512
          Dropout-36            [-1, 256, 8, 8]               0
           Conv2d-37            [-1, 256, 8, 8]         589,824
      BatchNorm2d-38            [-1, 256, 8, 8]             512
          Dropout-39            [-1, 256, 8, 8]               0
           Conv2d-40            [-1, 256, 8, 8]          32,768
      BatchNorm2d-41            [-1, 256, 8, 8]             512
          Dropout-42            [-1, 256, 8, 8]               0
       BasicBlock-43            [-1, 256, 8, 8]               0
           Conv2d-44            [-1, 256, 8, 8]         589,824
      BatchNorm2d-45            [-1, 256, 8, 8]             512
          Dropout-46            [-1, 256, 8, 8]               0
           Conv2d-47            [-1, 256, 8, 8]         589,824
      BatchNorm2d-48            [-1, 256, 8, 8]             512
          Dropout-49            [-1, 256, 8, 8]               0
       BasicBlock-50            [-1, 256, 8, 8]               0
           Conv2d-51            [-1, 512, 4, 4]       1,179,648
      BatchNorm2d-52            [-1, 512, 4, 4]           1,024
          Dropout-53            [-1, 512, 4, 4]               0
           Conv2d-54            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-55            [-1, 512, 4, 4]           1,024
          Dropout-56            [-1, 512, 4, 4]               0
           Conv2d-57            [-1, 512, 4, 4]         131,072
      BatchNorm2d-58            [-1, 512, 4, 4]           1,024
          Dropout-59            [-1, 512, 4, 4]               0
       BasicBlock-60            [-1, 512, 4, 4]               0
           Conv2d-61            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-62            [-1, 512, 4, 4]           1,024
          Dropout-63            [-1, 512, 4, 4]               0
           Conv2d-64            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-65            [-1, 512, 4, 4]           1,024
          Dropout-66            [-1, 512, 4, 4]               0
       BasicBlock-67            [-1, 512, 4, 4]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,173,962
Trainable params: 11,173,962
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 15.44
Params size (MB): 42.63
Estimated Total Size (MB): 58.07
----------------------------------------------------------------
Running with Optimizer: SGD, Learning Rate: 0.01
EPOCHS : 0

Test set: Average loss: 1.3995, Accuracy: 4917/10000 (49.17%)

EPOCHS : 1

Test set: Average loss: 1.1985, Accuracy: 5929/10000 (59.29%)

EPOCHS : 2

Test set: Average loss: 1.0006, Accuracy: 6488/10000 (64.88%)

EPOCHS : 3

Test set: Average loss: 1.0739, Accuracy: 6492/10000 (64.92%)

EPOCHS : 4

Test set: Average loss: 0.7271, Accuracy: 7474/10000 (74.74%)

EPOCHS : 5

Test set: Average loss: 0.8029, Accuracy: 7375/10000 (73.75%)

EPOCHS : 6

Test set: Average loss: 0.7440, Accuracy: 7594/10000 (75.94%)

EPOCHS : 7

Test set: Average loss: 0.5907, Accuracy: 7986/10000 (79.86%)

EPOCHS : 8

Test set: Average loss: 0.5917, Accuracy: 8010/10000 (80.10%)

EPOCHS : 9

Test set: Average loss: 0.5176, Accuracy: 8264/10000 (82.64%)

EPOCHS : 10

Test set: Average loss: 0.5786, Accuracy: 8156/10000 (81.56%)

EPOCHS : 11

Test set: Average loss: 0.5486, Accuracy: 8224/10000 (82.24%)

EPOCHS : 12

Test set: Average loss: 0.4900, Accuracy: 8365/10000 (83.65%)

EPOCHS : 13

Test set: Average loss: 0.4823, Accuracy: 8452/10000 (84.52%)

EPOCHS : 14

Test set: Average loss: 0.4761, Accuracy: 8429/10000 (84.29%)

EPOCHS : 15

Test set: Average loss: 0.4458, Accuracy: 8557/10000 (85.57%)

EPOCHS : 16

Test set: Average loss: 0.4416, Accuracy: 8577/10000 (85.77%)

EPOCHS : 17

Test set: Average loss: 0.4889, Accuracy: 8482/10000 (84.82%)

EPOCHS : 18

Test set: Average loss: 0.3997, Accuracy: 8703/10000 (87.03%)

EPOCHS : 19

Test set: Average loss: 0.4848, Accuracy: 8475/10000 (84.75%)

EPOCHS : 20

Test set: Average loss: 0.4487, Accuracy: 8619/10000 (86.19%)

EPOCHS : 21

Test set: Average loss: 0.4106, Accuracy: 8688/10000 (86.88%)

EPOCHS : 22

Test set: Average loss: 0.4254, Accuracy: 8665/10000 (86.65%)

EPOCHS : 23

Test set: Average loss: 0.4027, Accuracy: 8757/10000 (87.57%)

EPOCHS : 24

Test set: Average loss: 0.4333, Accuracy: 8692/10000 (86.92%)

EPOCHS : 25

Test set: Average loss: 0.4183, Accuracy: 8748/10000 (87.48%)

EPOCHS : 26

Test set: Average loss: 0.4170, Accuracy: 8746/10000 (87.46%)

EPOCHS : 27

Test set: Average loss: 0.3950, Accuracy: 8787/10000 (87.87%)

EPOCHS : 28

Test set: Average loss: 0.4405, Accuracy: 8688/10000 (86.88%)

EPOCHS : 29

Test set: Average loss: 0.4172, Accuracy: 8746/10000 (87.46%)

EPOCHS : 30
Epoch 00031: reducing learning rate of group 0 to 5.0000e-04.

Test set: Average loss: 0.3959, Accuracy: 8821/10000 (88.21%)

EPOCHS : 31

Test set: Average loss: 0.3456, Accuracy: 8948/10000 (89.48%)

EPOCHS : 32

Test set: Average loss: 0.3407, Accuracy: 8973/10000 (89.73%)

EPOCHS : 33

Test set: Average loss: 0.3402, Accuracy: 8974/10000 (89.74%)

EPOCHS : 34
Epoch 00035: reducing learning rate of group 0 to 2.5000e-05.

Test set: Average loss: 0.3374, Accuracy: 8983/10000 (89.83%)

EPOCHS : 35

Test set: Average loss: 0.3358, Accuracy: 8985/10000 (89.85%)

EPOCHS : 36

Test set: Average loss: 0.3352, Accuracy: 8988/10000 (89.88%)

EPOCHS : 37

Test set: Average loss: 0.3363, Accuracy: 8990/10000 (89.90%)

EPOCHS : 38
Epoch 00039: reducing learning rate of group 0 to 1.2500e-06.

Test set: Average loss: 0.3357, Accuracy: 8988/10000 (89.88%)

EPOCHS : 39

Test set: Average loss: 0.3357, Accuracy: 8986/10000 (89.86%)

Optimizer: SGD, Learning Rate: 0.01, Test Accuracy: 89.90
cuda
Files already downloaded and verified
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]          36,864
       BatchNorm2d-4           [-1, 64, 32, 32]             128
           Dropout-5           [-1, 64, 32, 32]               0
            Conv2d-6           [-1, 64, 32, 32]          36,864
       BatchNorm2d-7           [-1, 64, 32, 32]             128
           Dropout-8           [-1, 64, 32, 32]               0
        BasicBlock-9           [-1, 64, 32, 32]               0
           Conv2d-10           [-1, 64, 32, 32]          36,864
      BatchNorm2d-11           [-1, 64, 32, 32]             128
          Dropout-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 64, 32, 32]          36,864
      BatchNorm2d-14           [-1, 64, 32, 32]             128
          Dropout-15           [-1, 64, 32, 32]               0
       BasicBlock-16           [-1, 64, 32, 32]               0
           Conv2d-17          [-1, 128, 16, 16]          73,728
      BatchNorm2d-18          [-1, 128, 16, 16]             256
          Dropout-19          [-1, 128, 16, 16]               0
           Conv2d-20          [-1, 128, 16, 16]         147,456
      BatchNorm2d-21          [-1, 128, 16, 16]             256
          Dropout-22          [-1, 128, 16, 16]               0
           Conv2d-23          [-1, 128, 16, 16]           8,192
      BatchNorm2d-24          [-1, 128, 16, 16]             256
          Dropout-25          [-1, 128, 16, 16]               0
       BasicBlock-26          [-1, 128, 16, 16]               0
           Conv2d-27          [-1, 128, 16, 16]         147,456
      BatchNorm2d-28          [-1, 128, 16, 16]             256
          Dropout-29          [-1, 128, 16, 16]               0
           Conv2d-30          [-1, 128, 16, 16]         147,456
      BatchNorm2d-31          [-1, 128, 16, 16]             256
          Dropout-32          [-1, 128, 16, 16]               0
       BasicBlock-33          [-1, 128, 16, 16]               0
           Conv2d-34            [-1, 256, 8, 8]         294,912
      BatchNorm2d-35            [-1, 256, 8, 8]             512
          Dropout-36            [-1, 256, 8, 8]               0
           Conv2d-37            [-1, 256, 8, 8]         589,824
      BatchNorm2d-38            [-1, 256, 8, 8]             512
          Dropout-39            [-1, 256, 8, 8]               0
           Conv2d-40            [-1, 256, 8, 8]          32,768
      BatchNorm2d-41            [-1, 256, 8, 8]             512
          Dropout-42            [-1, 256, 8, 8]               0
       BasicBlock-43            [-1, 256, 8, 8]               0
           Conv2d-44            [-1, 256, 8, 8]         589,824
      BatchNorm2d-45            [-1, 256, 8, 8]             512
          Dropout-46            [-1, 256, 8, 8]               0
           Conv2d-47            [-1, 256, 8, 8]         589,824
      BatchNorm2d-48            [-1, 256, 8, 8]             512
          Dropout-49            [-1, 256, 8, 8]               0
       BasicBlock-50            [-1, 256, 8, 8]               0
           Conv2d-51            [-1, 512, 4, 4]       1,179,648
      BatchNorm2d-52            [-1, 512, 4, 4]           1,024
          Dropout-53            [-1, 512, 4, 4]               0
           Conv2d-54            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-55            [-1, 512, 4, 4]           1,024
          Dropout-56            [-1, 512, 4, 4]               0
           Conv2d-57            [-1, 512, 4, 4]         131,072
      BatchNorm2d-58            [-1, 512, 4, 4]           1,024
          Dropout-59            [-1, 512, 4, 4]               0
       BasicBlock-60            [-1, 512, 4, 4]               0
           Conv2d-61            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-62            [-1, 512, 4, 4]           1,024
          Dropout-63            [-1, 512, 4, 4]               0
           Conv2d-64            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-65            [-1, 512, 4, 4]           1,024
          Dropout-66            [-1, 512, 4, 4]               0
       BasicBlock-67            [-1, 512, 4, 4]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,173,962
Trainable params: 11,173,962
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 15.44
Params size (MB): 42.63
Estimated Total Size (MB): 58.07
----------------------------------------------------------------
Running with Optimizer: SGD, Learning Rate: 0.05
EPOCHS : 0

Test set: Average loss: 1.5051, Accuracy: 4774/10000 (47.74%)

EPOCHS : 1

Test set: Average loss: 1.3627, Accuracy: 5501/10000 (55.01%)

EPOCHS : 2

Test set: Average loss: 0.8864, Accuracy: 6829/10000 (68.29%)

EPOCHS : 3

Test set: Average loss: 0.8958, Accuracy: 6950/10000 (69.50%)

EPOCHS : 4

Test set: Average loss: 0.8485, Accuracy: 7241/10000 (72.41%)

EPOCHS : 5

Test set: Average loss: 0.7349, Accuracy: 7620/10000 (76.20%)

EPOCHS : 6

Test set: Average loss: 0.6164, Accuracy: 7948/10000 (79.48%)

EPOCHS : 7

Test set: Average loss: 0.5910, Accuracy: 8049/10000 (80.49%)

EPOCHS : 8

Test set: Average loss: 0.5751, Accuracy: 8132/10000 (81.32%)

EPOCHS : 9

Test set: Average loss: 0.5168, Accuracy: 8332/10000 (83.32%)

EPOCHS : 10

Test set: Average loss: 0.4586, Accuracy: 8460/10000 (84.60%)

EPOCHS : 11

Test set: Average loss: 0.4975, Accuracy: 8371/10000 (83.71%)

EPOCHS : 12

Test set: Average loss: 0.5300, Accuracy: 8336/10000 (83.36%)

EPOCHS : 13

Test set: Average loss: 0.4458, Accuracy: 8542/10000 (85.42%)

EPOCHS : 14

Test set: Average loss: 0.4436, Accuracy: 8560/10000 (85.60%)

EPOCHS : 15

Test set: Average loss: 0.5928, Accuracy: 8299/10000 (82.99%)

EPOCHS : 16

Test set: Average loss: 0.4194, Accuracy: 8690/10000 (86.90%)

EPOCHS : 17

Test set: Average loss: 0.4398, Accuracy: 8585/10000 (85.85%)

EPOCHS : 18

Test set: Average loss: 0.5019, Accuracy: 8510/10000 (85.10%)

EPOCHS : 19

Test set: Average loss: 0.4309, Accuracy: 8705/10000 (87.05%)

EPOCHS : 20

Test set: Average loss: 0.4132, Accuracy: 8720/10000 (87.20%)

EPOCHS : 21

Test set: Average loss: 0.4526, Accuracy: 8652/10000 (86.52%)

EPOCHS : 22

Test set: Average loss: 0.4241, Accuracy: 8733/10000 (87.33%)

EPOCHS : 23

Test set: Average loss: 0.4106, Accuracy: 8758/10000 (87.58%)

EPOCHS : 24
Epoch 00025: reducing learning rate of group 0 to 2.5000e-03.

Test set: Average loss: 0.5265, Accuracy: 8554/10000 (85.54%)

EPOCHS : 25

Test set: Average loss: 0.3208, Accuracy: 9024/10000 (90.24%)

EPOCHS : 26

Test set: Average loss: 0.3173, Accuracy: 9055/10000 (90.55%)

EPOCHS : 27

Test set: Average loss: 0.3178, Accuracy: 9044/10000 (90.44%)

EPOCHS : 28

Test set: Average loss: 0.3166, Accuracy: 9070/10000 (90.70%)

EPOCHS : 29

Test set: Average loss: 0.3176, Accuracy: 9067/10000 (90.67%)

EPOCHS : 30
Epoch 00031: reducing learning rate of group 0 to 1.2500e-04.

Test set: Average loss: 0.3159, Accuracy: 9078/10000 (90.78%)

EPOCHS : 31

Test set: Average loss: 0.3164, Accuracy: 9071/10000 (90.71%)

EPOCHS : 32

Test set: Average loss: 0.3158, Accuracy: 9075/10000 (90.75%)

EPOCHS : 33
Epoch 00034: reducing learning rate of group 0 to 6.2500e-06.

Test set: Average loss: 0.3166, Accuracy: 9074/10000 (90.74%)

EPOCHS : 34

Test set: Average loss: 0.3158, Accuracy: 9072/10000 (90.72%)

EPOCHS : 35

Test set: Average loss: 0.3157, Accuracy: 9080/10000 (90.80%)

EPOCHS : 36

Test set: Average loss: 0.3153, Accuracy: 9074/10000 (90.74%)

EPOCHS : 37

Test set: Average loss: 0.3169, Accuracy: 9080/10000 (90.80%)

EPOCHS : 38
Epoch 00039: reducing learning rate of group 0 to 3.1250e-07.

Test set: Average loss: 0.3167, Accuracy: 9077/10000 (90.77%)

EPOCHS : 39

Test set: Average loss: 0.3162, Accuracy: 9077/10000 (90.77%)

Optimizer: SGD, Learning Rate: 0.05, Test Accuracy: 90.80
cuda
Files already downloaded and verified
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]          36,864
       BatchNorm2d-4           [-1, 64, 32, 32]             128
           Dropout-5           [-1, 64, 32, 32]               0
            Conv2d-6           [-1, 64, 32, 32]          36,864
       BatchNorm2d-7           [-1, 64, 32, 32]             128
           Dropout-8           [-1, 64, 32, 32]               0
        BasicBlock-9           [-1, 64, 32, 32]               0
           Conv2d-10           [-1, 64, 32, 32]          36,864
      BatchNorm2d-11           [-1, 64, 32, 32]             128
          Dropout-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 64, 32, 32]          36,864
      BatchNorm2d-14           [-1, 64, 32, 32]             128
          Dropout-15           [-1, 64, 32, 32]               0
       BasicBlock-16           [-1, 64, 32, 32]               0
           Conv2d-17          [-1, 128, 16, 16]          73,728
      BatchNorm2d-18          [-1, 128, 16, 16]             256
          Dropout-19          [-1, 128, 16, 16]               0
           Conv2d-20          [-1, 128, 16, 16]         147,456
      BatchNorm2d-21          [-1, 128, 16, 16]             256
          Dropout-22          [-1, 128, 16, 16]               0
           Conv2d-23          [-1, 128, 16, 16]           8,192
      BatchNorm2d-24          [-1, 128, 16, 16]             256
          Dropout-25          [-1, 128, 16, 16]               0
       BasicBlock-26          [-1, 128, 16, 16]               0
           Conv2d-27          [-1, 128, 16, 16]         147,456
      BatchNorm2d-28          [-1, 128, 16, 16]             256
          Dropout-29          [-1, 128, 16, 16]               0
           Conv2d-30          [-1, 128, 16, 16]         147,456
      BatchNorm2d-31          [-1, 128, 16, 16]             256
          Dropout-32          [-1, 128, 16, 16]               0
       BasicBlock-33          [-1, 128, 16, 16]               0
           Conv2d-34            [-1, 256, 8, 8]         294,912
      BatchNorm2d-35            [-1, 256, 8, 8]             512
          Dropout-36            [-1, 256, 8, 8]               0
           Conv2d-37            [-1, 256, 8, 8]         589,824
      BatchNorm2d-38            [-1, 256, 8, 8]             512
          Dropout-39            [-1, 256, 8, 8]               0
           Conv2d-40            [-1, 256, 8, 8]          32,768
      BatchNorm2d-41            [-1, 256, 8, 8]             512
          Dropout-42            [-1, 256, 8, 8]               0
       BasicBlock-43            [-1, 256, 8, 8]               0
           Conv2d-44            [-1, 256, 8, 8]         589,824
      BatchNorm2d-45            [-1, 256, 8, 8]             512
          Dropout-46            [-1, 256, 8, 8]               0
           Conv2d-47            [-1, 256, 8, 8]         589,824
      BatchNorm2d-48            [-1, 256, 8, 8]             512
          Dropout-49            [-1, 256, 8, 8]               0
       BasicBlock-50            [-1, 256, 8, 8]               0
           Conv2d-51            [-1, 512, 4, 4]       1,179,648
      BatchNorm2d-52            [-1, 512, 4, 4]           1,024
          Dropout-53            [-1, 512, 4, 4]               0
           Conv2d-54            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-55            [-1, 512, 4, 4]           1,024
          Dropout-56            [-1, 512, 4, 4]               0
           Conv2d-57            [-1, 512, 4, 4]         131,072
      BatchNorm2d-58            [-1, 512, 4, 4]           1,024
          Dropout-59            [-1, 512, 4, 4]               0
       BasicBlock-60            [-1, 512, 4, 4]               0
           Conv2d-61            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-62            [-1, 512, 4, 4]           1,024
          Dropout-63            [-1, 512, 4, 4]               0
           Conv2d-64            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-65            [-1, 512, 4, 4]           1,024
          Dropout-66            [-1, 512, 4, 4]               0
       BasicBlock-67            [-1, 512, 4, 4]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,173,962
Trainable params: 11,173,962
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 15.44
Params size (MB): 42.63
Estimated Total Size (MB): 58.07
----------------------------------------------------------------
Running with Optimizer: Adam, Learning Rate: 0.005
EPOCHS : 0

Test set: Average loss: 1.5845, Accuracy: 4019/10000 (40.19%)

EPOCHS : 1

Test set: Average loss: 1.3058, Accuracy: 5340/10000 (53.40%)

EPOCHS : 2

Test set: Average loss: 1.2659, Accuracy: 5627/10000 (56.27%)

EPOCHS : 3

Test set: Average loss: 0.9829, Accuracy: 6551/10000 (65.51%)

EPOCHS : 4

Test set: Average loss: 1.2222, Accuracy: 6183/10000 (61.83%)

EPOCHS : 5

Test set: Average loss: 0.9031, Accuracy: 6914/10000 (69.14%)

EPOCHS : 6

Test set: Average loss: 0.7367, Accuracy: 7410/10000 (74.10%)

EPOCHS : 7

Test set: Average loss: 0.6778, Accuracy: 7665/10000 (76.65%)

EPOCHS : 8

Test set: Average loss: 0.5644, Accuracy: 8034/10000 (80.34%)

EPOCHS : 9

Test set: Average loss: 0.5998, Accuracy: 8002/10000 (80.02%)

EPOCHS : 10

Test set: Average loss: 0.5315, Accuracy: 8213/10000 (82.13%)

EPOCHS : 11

Test set: Average loss: 0.4560, Accuracy: 8491/10000 (84.91%)

EPOCHS : 12

Test set: Average loss: 0.5146, Accuracy: 8320/10000 (83.20%)

EPOCHS : 13

Test set: Average loss: 0.4696, Accuracy: 8458/10000 (84.58%)

EPOCHS : 14

Test set: Average loss: 0.4665, Accuracy: 8477/10000 (84.77%)

EPOCHS : 15

Test set: Average loss: 0.5108, Accuracy: 8366/10000 (83.66%)

EPOCHS : 16

Test set: Average loss: 0.4944, Accuracy: 8475/10000 (84.75%)

EPOCHS : 17

Test set: Average loss: 0.4484, Accuracy: 8579/10000 (85.79%)

EPOCHS : 18

Test set: Average loss: 0.4102, Accuracy: 8676/10000 (86.76%)

EPOCHS : 19
Epoch 00020: reducing learning rate of group 0 to 2.5000e-04.

Test set: Average loss: 0.4033, Accuracy: 8707/10000 (87.07%)

EPOCHS : 20

Test set: Average loss: 0.3302, Accuracy: 8950/10000 (89.50%)

EPOCHS : 21

Test set: Average loss: 0.3271, Accuracy: 8971/10000 (89.71%)

EPOCHS : 22

Test set: Average loss: 0.3259, Accuracy: 8971/10000 (89.71%)

EPOCHS : 23

Test set: Average loss: 0.3258, Accuracy: 8997/10000 (89.97%)

EPOCHS : 24

Test set: Average loss: 0.3264, Accuracy: 9011/10000 (90.11%)

EPOCHS : 25

Test set: Average loss: 0.3286, Accuracy: 9008/10000 (90.08%)

EPOCHS : 26

Test set: Average loss: 0.3320, Accuracy: 9006/10000 (90.06%)

EPOCHS : 27

Test set: Average loss: 0.3384, Accuracy: 9006/10000 (90.06%)

EPOCHS : 28

Test set: Average loss: 0.3384, Accuracy: 9010/10000 (90.10%)

EPOCHS : 29
Epoch 00030: reducing learning rate of group 0 to 1.2500e-05.

Test set: Average loss: 0.3341, Accuracy: 9029/10000 (90.29%)

EPOCHS : 30

Test set: Average loss: 0.3351, Accuracy: 9030/10000 (90.30%)

EPOCHS : 31

Test set: Average loss: 0.3348, Accuracy: 9039/10000 (90.39%)

EPOCHS : 32
Epoch 00033: reducing learning rate of group 0 to 6.2500e-07.

Test set: Average loss: 0.3340, Accuracy: 9028/10000 (90.28%)

EPOCHS : 33

Test set: Average loss: 0.3351, Accuracy: 9033/10000 (90.33%)

EPOCHS : 34

Test set: Average loss: 0.3338, Accuracy: 9036/10000 (90.36%)

EPOCHS : 35

Test set: Average loss: 0.3367, Accuracy: 9037/10000 (90.37%)

EPOCHS : 36
Epoch 00037: reducing learning rate of group 0 to 3.1250e-08.

Test set: Average loss: 0.3331, Accuracy: 9033/10000 (90.33%)

EPOCHS : 37

Test set: Average loss: 0.3363, Accuracy: 9029/10000 (90.29%)

EPOCHS : 38

Test set: Average loss: 0.3367, Accuracy: 9037/10000 (90.37%)

EPOCHS : 39
Epoch 00040: reducing learning rate of group 0 to 1.5625e-09.

Test set: Average loss: 0.3345, Accuracy: 9031/10000 (90.31%)

Optimizer: Adam, Learning Rate: 0.005, Test Accuracy: 90.39
cuda
Files already downloaded and verified
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]          36,864
       BatchNorm2d-4           [-1, 64, 32, 32]             128
           Dropout-5           [-1, 64, 32, 32]               0
            Conv2d-6           [-1, 64, 32, 32]          36,864
       BatchNorm2d-7           [-1, 64, 32, 32]             128
           Dropout-8           [-1, 64, 32, 32]               0
        BasicBlock-9           [-1, 64, 32, 32]               0
           Conv2d-10           [-1, 64, 32, 32]          36,864
      BatchNorm2d-11           [-1, 64, 32, 32]             128
          Dropout-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 64, 32, 32]          36,864
      BatchNorm2d-14           [-1, 64, 32, 32]             128
          Dropout-15           [-1, 64, 32, 32]               0
       BasicBlock-16           [-1, 64, 32, 32]               0
           Conv2d-17          [-1, 128, 16, 16]          73,728
      BatchNorm2d-18          [-1, 128, 16, 16]             256
          Dropout-19          [-1, 128, 16, 16]               0
           Conv2d-20          [-1, 128, 16, 16]         147,456
      BatchNorm2d-21          [-1, 128, 16, 16]             256
          Dropout-22          [-1, 128, 16, 16]               0
           Conv2d-23          [-1, 128, 16, 16]           8,192
      BatchNorm2d-24          [-1, 128, 16, 16]             256
          Dropout-25          [-1, 128, 16, 16]               0
       BasicBlock-26          [-1, 128, 16, 16]               0
           Conv2d-27          [-1, 128, 16, 16]         147,456
      BatchNorm2d-28          [-1, 128, 16, 16]             256
          Dropout-29          [-1, 128, 16, 16]               0
           Conv2d-30          [-1, 128, 16, 16]         147,456
      BatchNorm2d-31          [-1, 128, 16, 16]             256
          Dropout-32          [-1, 128, 16, 16]               0
       BasicBlock-33          [-1, 128, 16, 16]               0
           Conv2d-34            [-1, 256, 8, 8]         294,912
      BatchNorm2d-35            [-1, 256, 8, 8]             512
          Dropout-36            [-1, 256, 8, 8]               0
           Conv2d-37            [-1, 256, 8, 8]         589,824
      BatchNorm2d-38            [-1, 256, 8, 8]             512
          Dropout-39            [-1, 256, 8, 8]               0
           Conv2d-40            [-1, 256, 8, 8]          32,768
      BatchNorm2d-41            [-1, 256, 8, 8]             512
          Dropout-42            [-1, 256, 8, 8]               0
       BasicBlock-43            [-1, 256, 8, 8]               0
           Conv2d-44            [-1, 256, 8, 8]         589,824
      BatchNorm2d-45            [-1, 256, 8, 8]             512
          Dropout-46            [-1, 256, 8, 8]               0
           Conv2d-47            [-1, 256, 8, 8]         589,824
      BatchNorm2d-48            [-1, 256, 8, 8]             512
          Dropout-49            [-1, 256, 8, 8]               0
       BasicBlock-50            [-1, 256, 8, 8]               0
           Conv2d-51            [-1, 512, 4, 4]       1,179,648
      BatchNorm2d-52            [-1, 512, 4, 4]           1,024
          Dropout-53            [-1, 512, 4, 4]               0
           Conv2d-54            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-55            [-1, 512, 4, 4]           1,024
          Dropout-56            [-1, 512, 4, 4]               0
           Conv2d-57            [-1, 512, 4, 4]         131,072
      BatchNorm2d-58            [-1, 512, 4, 4]           1,024
          Dropout-59            [-1, 512, 4, 4]               0
       BasicBlock-60            [-1, 512, 4, 4]               0
           Conv2d-61            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-62            [-1, 512, 4, 4]           1,024
          Dropout-63            [-1, 512, 4, 4]               0
           Conv2d-64            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-65            [-1, 512, 4, 4]           1,024
          Dropout-66            [-1, 512, 4, 4]               0
       BasicBlock-67            [-1, 512, 4, 4]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,173,962
Trainable params: 11,173,962
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 15.44
Params size (MB): 42.63
Estimated Total Size (MB): 58.07
----------------------------------------------------------------
Running with Optimizer: Adam, Learning Rate: 0.01
EPOCHS : 0

Test set: Average loss: 1.7477, Accuracy: 3532/10000 (35.32%)

EPOCHS : 1

Test set: Average loss: 1.4407, Accuracy: 4720/10000 (47.20%)

EPOCHS : 2

Test set: Average loss: 1.2804, Accuracy: 5324/10000 (53.24%)

EPOCHS : 3

Test set: Average loss: 1.0689, Accuracy: 6201/10000 (62.01%)

EPOCHS : 4

Test set: Average loss: 0.9376, Accuracy: 6683/10000 (66.83%)

EPOCHS : 5

Test set: Average loss: 0.9907, Accuracy: 6612/10000 (66.12%)

EPOCHS : 6

Test set: Average loss: 0.8301, Accuracy: 7160/10000 (71.60%)

EPOCHS : 7

Test set: Average loss: 0.6989, Accuracy: 7621/10000 (76.21%)

EPOCHS : 8

Test set: Average loss: 0.7661, Accuracy: 7453/10000 (74.53%)

EPOCHS : 9

Test set: Average loss: 0.6018, Accuracy: 7974/10000 (79.74%)

EPOCHS : 10

Test set: Average loss: 0.6070, Accuracy: 8015/10000 (80.15%)

EPOCHS : 11

Test set: Average loss: 0.5334, Accuracy: 8207/10000 (82.07%)

EPOCHS : 12

Test set: Average loss: 0.5238, Accuracy: 8272/10000 (82.72%)

EPOCHS : 13

Test set: Average loss: 0.4870, Accuracy: 8391/10000 (83.91%)

EPOCHS : 14

Test set: Average loss: 0.5154, Accuracy: 8342/10000 (83.42%)

EPOCHS : 15

Test set: Average loss: 0.5010, Accuracy: 8412/10000 (84.12%)

EPOCHS : 16

Test set: Average loss: 0.4541, Accuracy: 8538/10000 (85.38%)

EPOCHS : 17

Test set: Average loss: 0.4790, Accuracy: 8508/10000 (85.08%)

EPOCHS : 18

Test set: Average loss: 0.4336, Accuracy: 8624/10000 (86.24%)

EPOCHS : 19

Test set: Average loss: 0.5021, Accuracy: 8461/10000 (84.61%)

EPOCHS : 20

Test set: Average loss: 0.4509, Accuracy: 8668/10000 (86.68%)

EPOCHS : 21

Test set: Average loss: 0.4619, Accuracy: 8610/10000 (86.10%)

EPOCHS : 22

Test set: Average loss: 0.4662, Accuracy: 8609/10000 (86.09%)

EPOCHS : 23

Test set: Average loss: 0.4704, Accuracy: 8590/10000 (85.90%)

EPOCHS : 24

Test set: Average loss: 0.4822, Accuracy: 8624/10000 (86.24%)

EPOCHS : 25

Test set: Average loss: 0.4785, Accuracy: 8639/10000 (86.39%)

EPOCHS : 26
Epoch 00027: reducing learning rate of group 0 to 5.0000e-04.

Test set: Average loss: 0.4476, Accuracy: 8716/10000 (87.16%)

EPOCHS : 27

Test set: Average loss: 0.3812, Accuracy: 8897/10000 (88.97%)

EPOCHS : 28

Test set: Average loss: 0.3776, Accuracy: 8940/10000 (89.40%)

EPOCHS : 29

Test set: Average loss: 0.3790, Accuracy: 8946/10000 (89.46%)

EPOCHS : 30

Test set: Average loss: 0.3860, Accuracy: 8947/10000 (89.47%)

EPOCHS : 31

Test set: Average loss: 0.3834, Accuracy: 8957/10000 (89.57%)

EPOCHS : 32

Test set: Average loss: 0.3916, Accuracy: 8960/10000 (89.60%)

EPOCHS : 33

Test set: Average loss: 0.3928, Accuracy: 8973/10000 (89.73%)

EPOCHS : 34

Test set: Average loss: 0.3996, Accuracy: 8961/10000 (89.61%)

EPOCHS : 35

Test set: Average loss: 0.3993, Accuracy: 8978/10000 (89.78%)

EPOCHS : 36
Epoch 00037: reducing learning rate of group 0 to 2.5000e-05.

Test set: Average loss: 0.4029, Accuracy: 8985/10000 (89.85%)

EPOCHS : 37

Test set: Average loss: 0.4078, Accuracy: 8980/10000 (89.80%)

EPOCHS : 38

Test set: Average loss: 0.4035, Accuracy: 8988/10000 (89.88%)

EPOCHS : 39

Test set: Average loss: 0.4046, Accuracy: 8988/10000 (89.88%)

Optimizer: Adam, Learning Rate: 0.01, Test Accuracy: 89.88
cuda
Files already downloaded and verified
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]          36,864
       BatchNorm2d-4           [-1, 64, 32, 32]             128
           Dropout-5           [-1, 64, 32, 32]               0
            Conv2d-6           [-1, 64, 32, 32]          36,864
       BatchNorm2d-7           [-1, 64, 32, 32]             128
           Dropout-8           [-1, 64, 32, 32]               0
        BasicBlock-9           [-1, 64, 32, 32]               0
           Conv2d-10           [-1, 64, 32, 32]          36,864
      BatchNorm2d-11           [-1, 64, 32, 32]             128
          Dropout-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 64, 32, 32]          36,864
      BatchNorm2d-14           [-1, 64, 32, 32]             128
          Dropout-15           [-1, 64, 32, 32]               0
       BasicBlock-16           [-1, 64, 32, 32]               0
           Conv2d-17          [-1, 128, 16, 16]          73,728
      BatchNorm2d-18          [-1, 128, 16, 16]             256
          Dropout-19          [-1, 128, 16, 16]               0
           Conv2d-20          [-1, 128, 16, 16]         147,456
      BatchNorm2d-21          [-1, 128, 16, 16]             256
          Dropout-22          [-1, 128, 16, 16]               0
           Conv2d-23          [-1, 128, 16, 16]           8,192
      BatchNorm2d-24          [-1, 128, 16, 16]             256
          Dropout-25          [-1, 128, 16, 16]               0
       BasicBlock-26          [-1, 128, 16, 16]               0
           Conv2d-27          [-1, 128, 16, 16]         147,456
      BatchNorm2d-28          [-1, 128, 16, 16]             256
          Dropout-29          [-1, 128, 16, 16]               0
           Conv2d-30          [-1, 128, 16, 16]         147,456
      BatchNorm2d-31          [-1, 128, 16, 16]             256
          Dropout-32          [-1, 128, 16, 16]               0
       BasicBlock-33          [-1, 128, 16, 16]               0
           Conv2d-34            [-1, 256, 8, 8]         294,912
      BatchNorm2d-35            [-1, 256, 8, 8]             512
          Dropout-36            [-1, 256, 8, 8]               0
           Conv2d-37            [-1, 256, 8, 8]         589,824
      BatchNorm2d-38            [-1, 256, 8, 8]             512
          Dropout-39            [-1, 256, 8, 8]               0
           Conv2d-40            [-1, 256, 8, 8]          32,768
      BatchNorm2d-41            [-1, 256, 8, 8]             512
          Dropout-42            [-1, 256, 8, 8]               0
       BasicBlock-43            [-1, 256, 8, 8]               0
           Conv2d-44            [-1, 256, 8, 8]         589,824
      BatchNorm2d-45            [-1, 256, 8, 8]             512
          Dropout-46            [-1, 256, 8, 8]               0
           Conv2d-47            [-1, 256, 8, 8]         589,824
      BatchNorm2d-48            [-1, 256, 8, 8]             512
          Dropout-49            [-1, 256, 8, 8]               0
       BasicBlock-50            [-1, 256, 8, 8]               0
           Conv2d-51            [-1, 512, 4, 4]       1,179,648
      BatchNorm2d-52            [-1, 512, 4, 4]           1,024
          Dropout-53            [-1, 512, 4, 4]               0
           Conv2d-54            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-55            [-1, 512, 4, 4]           1,024
          Dropout-56            [-1, 512, 4, 4]               0
           Conv2d-57            [-1, 512, 4, 4]         131,072
      BatchNorm2d-58            [-1, 512, 4, 4]           1,024
          Dropout-59            [-1, 512, 4, 4]               0
       BasicBlock-60            [-1, 512, 4, 4]               0
           Conv2d-61            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-62            [-1, 512, 4, 4]           1,024
          Dropout-63            [-1, 512, 4, 4]               0
           Conv2d-64            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-65            [-1, 512, 4, 4]           1,024
          Dropout-66            [-1, 512, 4, 4]               0
       BasicBlock-67            [-1, 512, 4, 4]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,173,962
Trainable params: 11,173,962
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 15.44
Params size (MB): 42.63
Estimated Total Size (MB): 58.07
----------------------------------------------------------------
Running with Optimizer: Adam, Learning Rate: 0.05
EPOCHS : 0

Test set: Average loss: 2.1318, Accuracy: 2082/10000 (20.82%)

EPOCHS : 1

Test set: Average loss: 1.8621, Accuracy: 3069/10000 (30.69%)

EPOCHS : 2

Test set: Average loss: 1.9639, Accuracy: 3435/10000 (34.35%)

EPOCHS : 3

Test set: Average loss: 1.5884, Accuracy: 4334/10000 (43.34%)

EPOCHS : 4

Test set: Average loss: 1.4323, Accuracy: 4866/10000 (48.66%)

EPOCHS : 5

Test set: Average loss: 1.2339, Accuracy: 5553/10000 (55.53%)

EPOCHS : 6

Test set: Average loss: 1.0691, Accuracy: 6154/10000 (61.54%)

EPOCHS : 7

Test set: Average loss: 1.0182, Accuracy: 6489/10000 (64.89%)

EPOCHS : 8

Test set: Average loss: 1.0103, Accuracy: 6562/10000 (65.62%)

EPOCHS : 9

Test set: Average loss: 0.9076, Accuracy: 6896/10000 (68.96%)

EPOCHS : 10

Test set: Average loss: 0.8150, Accuracy: 7209/10000 (72.09%)

EPOCHS : 11

Test set: Average loss: 0.7781, Accuracy: 7396/10000 (73.96%)

EPOCHS : 12

Test set: Average loss: 0.6942, Accuracy: 7664/10000 (76.64%)

EPOCHS : 13

Test set: Average loss: 0.7110, Accuracy: 7660/10000 (76.60%)

EPOCHS : 14

Test set: Average loss: 0.6271, Accuracy: 7904/10000 (79.04%)

EPOCHS : 15

Test set: Average loss: 0.5661, Accuracy: 8077/10000 (80.77%)

EPOCHS : 16

Test set: Average loss: 0.6037, Accuracy: 8015/10000 (80.15%)

EPOCHS : 17

Test set: Average loss: 0.5748, Accuracy: 8127/10000 (81.27%)

EPOCHS : 18

Test set: Average loss: 0.5584, Accuracy: 8158/10000 (81.58%)

EPOCHS : 19

Test set: Average loss: 0.5079, Accuracy: 8300/10000 (83.00%)

EPOCHS : 20

Test set: Average loss: 0.5533, Accuracy: 8249/10000 (82.49%)

EPOCHS : 21

Test set: Average loss: 0.4829, Accuracy: 8438/10000 (84.38%)

EPOCHS : 22

Test set: Average loss: 0.4682, Accuracy: 8472/10000 (84.72%)

EPOCHS : 23

Test set: Average loss: 0.5270, Accuracy: 8343/10000 (83.43%)

EPOCHS : 24
Epoch 00025: reducing learning rate of group 0 to 2.5000e-03.

Test set: Average loss: 0.4548, Accuracy: 8506/10000 (85.06%)

EPOCHS : 25

Test set: Average loss: 0.3855, Accuracy: 8720/10000 (87.20%)

EPOCHS : 26

Test set: Average loss: 0.3805, Accuracy: 8755/10000 (87.55%)

EPOCHS : 27

Test set: Average loss: 0.3792, Accuracy: 8783/10000 (87.83%)

EPOCHS : 28

Test set: Average loss: 0.3807, Accuracy: 8787/10000 (87.87%)

EPOCHS : 29

Test set: Average loss: 0.3751, Accuracy: 8803/10000 (88.03%)

EPOCHS : 30

Test set: Average loss: 0.3777, Accuracy: 8817/10000 (88.17%)

EPOCHS : 31

Test set: Average loss: 0.3732, Accuracy: 8818/10000 (88.18%)

EPOCHS : 32

Test set: Average loss: 0.3777, Accuracy: 8815/10000 (88.15%)

EPOCHS : 33

Test set: Average loss: 0.3768, Accuracy: 8820/10000 (88.20%)

EPOCHS : 34

Test set: Average loss: 0.3803, Accuracy: 8786/10000 (87.86%)

EPOCHS : 35

Test set: Average loss: 0.3785, Accuracy: 8816/10000 (88.16%)

EPOCHS : 36

Test set: Average loss: 0.3825, Accuracy: 8822/10000 (88.22%)

EPOCHS : 37

Test set: Average loss: 0.3822, Accuracy: 8815/10000 (88.15%)

EPOCHS : 38

Test set: Average loss: 0.3859, Accuracy: 8801/10000 (88.01%)

EPOCHS : 39

Test set: Average loss: 0.3788, Accuracy: 8854/10000 (88.54%)

Optimizer: Adam, Learning Rate: 0.05, Test Accuracy: 88.54
cuda
Files already downloaded and verified
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]          36,864
       BatchNorm2d-4           [-1, 64, 32, 32]             128
           Dropout-5           [-1, 64, 32, 32]               0
            Conv2d-6           [-1, 64, 32, 32]          36,864
       BatchNorm2d-7           [-1, 64, 32, 32]             128
           Dropout-8           [-1, 64, 32, 32]               0
        BasicBlock-9           [-1, 64, 32, 32]               0
           Conv2d-10           [-1, 64, 32, 32]          36,864
      BatchNorm2d-11           [-1, 64, 32, 32]             128
          Dropout-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 64, 32, 32]          36,864
      BatchNorm2d-14           [-1, 64, 32, 32]             128
          Dropout-15           [-1, 64, 32, 32]               0
       BasicBlock-16           [-1, 64, 32, 32]               0
           Conv2d-17          [-1, 128, 16, 16]          73,728
      BatchNorm2d-18          [-1, 128, 16, 16]             256
          Dropout-19          [-1, 128, 16, 16]               0
           Conv2d-20          [-1, 128, 16, 16]         147,456
      BatchNorm2d-21          [-1, 128, 16, 16]             256
          Dropout-22          [-1, 128, 16, 16]               0
           Conv2d-23          [-1, 128, 16, 16]           8,192
      BatchNorm2d-24          [-1, 128, 16, 16]             256
          Dropout-25          [-1, 128, 16, 16]               0
       BasicBlock-26          [-1, 128, 16, 16]               0
           Conv2d-27          [-1, 128, 16, 16]         147,456
      BatchNorm2d-28          [-1, 128, 16, 16]             256
          Dropout-29          [-1, 128, 16, 16]               0
           Conv2d-30          [-1, 128, 16, 16]         147,456
      BatchNorm2d-31          [-1, 128, 16, 16]             256
          Dropout-32          [-1, 128, 16, 16]               0
       BasicBlock-33          [-1, 128, 16, 16]               0
           Conv2d-34            [-1, 256, 8, 8]         294,912
      BatchNorm2d-35            [-1, 256, 8, 8]             512
          Dropout-36            [-1, 256, 8, 8]               0
           Conv2d-37            [-1, 256, 8, 8]         589,824
      BatchNorm2d-38            [-1, 256, 8, 8]             512
          Dropout-39            [-1, 256, 8, 8]               0
           Conv2d-40            [-1, 256, 8, 8]          32,768
      BatchNorm2d-41            [-1, 256, 8, 8]             512
          Dropout-42            [-1, 256, 8, 8]               0
       BasicBlock-43            [-1, 256, 8, 8]               0
           Conv2d-44            [-1, 256, 8, 8]         589,824
      BatchNorm2d-45            [-1, 256, 8, 8]             512
          Dropout-46            [-1, 256, 8, 8]               0
           Conv2d-47            [-1, 256, 8, 8]         589,824
      BatchNorm2d-48            [-1, 256, 8, 8]             512
          Dropout-49            [-1, 256, 8, 8]               0
       BasicBlock-50            [-1, 256, 8, 8]               0
           Conv2d-51            [-1, 512, 4, 4]       1,179,648
      BatchNorm2d-52            [-1, 512, 4, 4]           1,024
          Dropout-53            [-1, 512, 4, 4]               0
           Conv2d-54            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-55            [-1, 512, 4, 4]           1,024
          Dropout-56            [-1, 512, 4, 4]               0
           Conv2d-57            [-1, 512, 4, 4]         131,072
      BatchNorm2d-58            [-1, 512, 4, 4]           1,024
          Dropout-59            [-1, 512, 4, 4]               0
       BasicBlock-60            [-1, 512, 4, 4]               0
           Conv2d-61            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-62            [-1, 512, 4, 4]           1,024
          Dropout-63            [-1, 512, 4, 4]               0
           Conv2d-64            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-65            [-1, 512, 4, 4]           1,024
          Dropout-66            [-1, 512, 4, 4]               0
       BasicBlock-67            [-1, 512, 4, 4]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,173,962
Trainable params: 11,173,962
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 15.44
Params size (MB): 42.63
Estimated Total Size (MB): 58.07
----------------------------------------------------------------
Running with Optimizer: RMSprop, Learning Rate: 0.005
EPOCHS : 0

Test set: Average loss: 1.8596, Accuracy: 3230/10000 (32.30%)

EPOCHS : 1

Test set: Average loss: 1.5954, Accuracy: 4170/10000 (41.70%)

EPOCHS : 2

Test set: Average loss: 1.5260, Accuracy: 4473/10000 (44.73%)

EPOCHS : 3

Test set: Average loss: 1.2939, Accuracy: 5228/10000 (52.28%)

EPOCHS : 4

Test set: Average loss: 1.3639, Accuracy: 5104/10000 (51.04%)

EPOCHS : 5

Test set: Average loss: 1.0286, Accuracy: 6358/10000 (63.58%)

EPOCHS : 6

Test set: Average loss: 1.1563, Accuracy: 6158/10000 (61.58%)

EPOCHS : 7

Test set: Average loss: 0.9601, Accuracy: 6700/10000 (67.00%)

EPOCHS : 8

Test set: Average loss: 0.8556, Accuracy: 7023/10000 (70.23%)

EPOCHS : 9

Test set: Average loss: 0.9477, Accuracy: 6832/10000 (68.32%)

EPOCHS : 10

Test set: Average loss: 0.9260, Accuracy: 6885/10000 (68.85%)

EPOCHS : 11

Test set: Average loss: 0.6448, Accuracy: 7785/10000 (77.85%)

EPOCHS : 12

Test set: Average loss: 0.7900, Accuracy: 7466/10000 (74.66%)

EPOCHS : 13

Test set: Average loss: 0.9090, Accuracy: 7137/10000 (71.37%)

EPOCHS : 14

Test set: Average loss: 0.5779, Accuracy: 8032/10000 (80.32%)

EPOCHS : 15

Test set: Average loss: 0.5660, Accuracy: 8112/10000 (81.12%)

EPOCHS : 16

Test set: Average loss: 0.5937, Accuracy: 8121/10000 (81.21%)

EPOCHS : 17

Test set: Average loss: 0.4477, Accuracy: 8502/10000 (85.02%)

EPOCHS : 18

Test set: Average loss: 0.5425, Accuracy: 8303/10000 (83.03%)

EPOCHS : 19
Epoch 00020: reducing learning rate of group 0 to 2.5000e-04.

Test set: Average loss: 0.6811, Accuracy: 8130/10000 (81.30%)

EPOCHS : 20

Test set: Average loss: 0.3866, Accuracy: 8800/10000 (88.00%)

EPOCHS : 21

Test set: Average loss: 0.3895, Accuracy: 8817/10000 (88.17%)

EPOCHS : 22

Test set: Average loss: 0.3875, Accuracy: 8838/10000 (88.38%)

EPOCHS : 23

Test set: Average loss: 0.3919, Accuracy: 8858/10000 (88.58%)

EPOCHS : 24

Test set: Average loss: 0.3946, Accuracy: 8859/10000 (88.59%)

EPOCHS : 25

Test set: Average loss: 0.3954, Accuracy: 8863/10000 (88.63%)

EPOCHS : 26

Test set: Average loss: 0.3959, Accuracy: 8861/10000 (88.61%)

EPOCHS : 27
Epoch 00028: reducing learning rate of group 0 to 1.2500e-05.

Test set: Average loss: 0.3972, Accuracy: 8867/10000 (88.67%)

EPOCHS : 28

Test set: Average loss: 0.3976, Accuracy: 8863/10000 (88.63%)

EPOCHS : 29

Test set: Average loss: 0.3964, Accuracy: 8876/10000 (88.76%)

EPOCHS : 30
Epoch 00031: reducing learning rate of group 0 to 6.2500e-07.

Test set: Average loss: 0.3969, Accuracy: 8872/10000 (88.72%)

EPOCHS : 31

Test set: Average loss: 0.3984, Accuracy: 8868/10000 (88.68%)

EPOCHS : 32

Test set: Average loss: 0.4001, Accuracy: 8867/10000 (88.67%)

EPOCHS : 33

Test set: Average loss: 0.3969, Accuracy: 8872/10000 (88.72%)

EPOCHS : 34

Test set: Average loss: 0.3987, Accuracy: 8867/10000 (88.67%)

EPOCHS : 35

Test set: Average loss: 0.3997, Accuracy: 8874/10000 (88.74%)

EPOCHS : 36

Test set: Average loss: 0.3985, Accuracy: 8871/10000 (88.71%)

EPOCHS : 37
Epoch 00038: reducing learning rate of group 0 to 3.1250e-08.

Test set: Average loss: 0.3990, Accuracy: 8866/10000 (88.66%)

EPOCHS : 38

Test set: Average loss: 0.3974, Accuracy: 8865/10000 (88.65%)

EPOCHS : 39

Test set: Average loss: 0.3982, Accuracy: 8868/10000 (88.68%)

Optimizer: RMSprop, Learning Rate: 0.005, Test Accuracy: 88.76
cuda
Files already downloaded and verified
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]          36,864
       BatchNorm2d-4           [-1, 64, 32, 32]             128
           Dropout-5           [-1, 64, 32, 32]               0
            Conv2d-6           [-1, 64, 32, 32]          36,864
       BatchNorm2d-7           [-1, 64, 32, 32]             128
           Dropout-8           [-1, 64, 32, 32]               0
        BasicBlock-9           [-1, 64, 32, 32]               0
           Conv2d-10           [-1, 64, 32, 32]          36,864
      BatchNorm2d-11           [-1, 64, 32, 32]             128
          Dropout-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 64, 32, 32]          36,864
      BatchNorm2d-14           [-1, 64, 32, 32]             128
          Dropout-15           [-1, 64, 32, 32]               0
       BasicBlock-16           [-1, 64, 32, 32]               0
           Conv2d-17          [-1, 128, 16, 16]          73,728
      BatchNorm2d-18          [-1, 128, 16, 16]             256
          Dropout-19          [-1, 128, 16, 16]               0
           Conv2d-20          [-1, 128, 16, 16]         147,456
      BatchNorm2d-21          [-1, 128, 16, 16]             256
          Dropout-22          [-1, 128, 16, 16]               0
           Conv2d-23          [-1, 128, 16, 16]           8,192
      BatchNorm2d-24          [-1, 128, 16, 16]             256
          Dropout-25          [-1, 128, 16, 16]               0
       BasicBlock-26          [-1, 128, 16, 16]               0
           Conv2d-27          [-1, 128, 16, 16]         147,456
      BatchNorm2d-28          [-1, 128, 16, 16]             256
          Dropout-29          [-1, 128, 16, 16]               0
           Conv2d-30          [-1, 128, 16, 16]         147,456
      BatchNorm2d-31          [-1, 128, 16, 16]             256
          Dropout-32          [-1, 128, 16, 16]               0
       BasicBlock-33          [-1, 128, 16, 16]               0
           Conv2d-34            [-1, 256, 8, 8]         294,912
      BatchNorm2d-35            [-1, 256, 8, 8]             512
          Dropout-36            [-1, 256, 8, 8]               0
           Conv2d-37            [-1, 256, 8, 8]         589,824
      BatchNorm2d-38            [-1, 256, 8, 8]             512
          Dropout-39            [-1, 256, 8, 8]               0
           Conv2d-40            [-1, 256, 8, 8]          32,768
      BatchNorm2d-41            [-1, 256, 8, 8]             512
          Dropout-42            [-1, 256, 8, 8]               0
       BasicBlock-43            [-1, 256, 8, 8]               0
           Conv2d-44            [-1, 256, 8, 8]         589,824
      BatchNorm2d-45            [-1, 256, 8, 8]             512
          Dropout-46            [-1, 256, 8, 8]               0
           Conv2d-47            [-1, 256, 8, 8]         589,824
      BatchNorm2d-48            [-1, 256, 8, 8]             512
          Dropout-49            [-1, 256, 8, 8]               0
       BasicBlock-50            [-1, 256, 8, 8]               0
           Conv2d-51            [-1, 512, 4, 4]       1,179,648
      BatchNorm2d-52            [-1, 512, 4, 4]           1,024
          Dropout-53            [-1, 512, 4, 4]               0
           Conv2d-54            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-55            [-1, 512, 4, 4]           1,024
          Dropout-56            [-1, 512, 4, 4]               0
           Conv2d-57            [-1, 512, 4, 4]         131,072
      BatchNorm2d-58            [-1, 512, 4, 4]           1,024
          Dropout-59            [-1, 512, 4, 4]               0
       BasicBlock-60            [-1, 512, 4, 4]               0
           Conv2d-61            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-62            [-1, 512, 4, 4]           1,024
          Dropout-63            [-1, 512, 4, 4]               0
           Conv2d-64            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-65            [-1, 512, 4, 4]           1,024
          Dropout-66            [-1, 512, 4, 4]               0
       BasicBlock-67            [-1, 512, 4, 4]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,173,962
Trainable params: 11,173,962
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 15.44
Params size (MB): 42.63
Estimated Total Size (MB): 58.07
----------------------------------------------------------------
Running with Optimizer: RMSprop, Learning Rate: 0.01
EPOCHS : 0

Test set: Average loss: 1.9275, Accuracy: 2803/10000 (28.03%)

EPOCHS : 1

Test set: Average loss: 2.0083, Accuracy: 2829/10000 (28.29%)

EPOCHS : 2

Test set: Average loss: 1.8246, Accuracy: 3435/10000 (34.35%)

EPOCHS : 3

Test set: Average loss: 1.5687, Accuracy: 4472/10000 (44.72%)

EPOCHS : 4

Test set: Average loss: 1.3103, Accuracy: 5156/10000 (51.56%)

EPOCHS : 5

Test set: Average loss: 1.1919, Accuracy: 5749/10000 (57.49%)

EPOCHS : 6

Test set: Average loss: 1.2265, Accuracy: 5670/10000 (56.70%)

EPOCHS : 7

Test set: Average loss: 1.2847, Accuracy: 5655/10000 (56.55%)

EPOCHS : 8

Test set: Average loss: 1.0150, Accuracy: 6483/10000 (64.83%)

EPOCHS : 9

Test set: Average loss: 1.2803, Accuracy: 6061/10000 (60.61%)

EPOCHS : 10

Test set: Average loss: 0.8888, Accuracy: 7085/10000 (70.85%)

EPOCHS : 11

Test set: Average loss: 0.7099, Accuracy: 7607/10000 (76.07%)

EPOCHS : 12

Test set: Average loss: 0.8272, Accuracy: 7281/10000 (72.81%)

EPOCHS : 13

Test set: Average loss: 0.6303, Accuracy: 7859/10000 (78.59%)

EPOCHS : 14

Test set: Average loss: 0.5947, Accuracy: 8055/10000 (80.55%)

EPOCHS : 15

Test set: Average loss: 0.5984, Accuracy: 8055/10000 (80.55%)

EPOCHS : 16

Test set: Average loss: 0.7008, Accuracy: 7838/10000 (78.38%)

EPOCHS : 17

Test set: Average loss: 0.4645, Accuracy: 8459/10000 (84.59%)

EPOCHS : 18

Test set: Average loss: 0.5254, Accuracy: 8299/10000 (82.99%)

EPOCHS : 19

Test set: Average loss: 0.6528, Accuracy: 8060/10000 (80.60%)

EPOCHS : 20

Test set: Average loss: 0.5385, Accuracy: 8381/10000 (83.81%)

EPOCHS : 21

Test set: Average loss: 0.5446, Accuracy: 8353/10000 (83.53%)

EPOCHS : 22

Test set: Average loss: 0.5233, Accuracy: 8422/10000 (84.22%)

EPOCHS : 23

Test set: Average loss: 0.5446, Accuracy: 8457/10000 (84.57%)

EPOCHS : 24

Test set: Average loss: 0.5215, Accuracy: 8471/10000 (84.71%)

EPOCHS : 25

Test set: Average loss: 0.5119, Accuracy: 8559/10000 (85.59%)

EPOCHS : 26

Test set: Average loss: 0.4825, Accuracy: 8603/10000 (86.03%)

EPOCHS : 27

Test set: Average loss: 0.4692, Accuracy: 8672/10000 (86.72%)

EPOCHS : 28

Test set: Average loss: 0.5032, Accuracy: 8535/10000 (85.35%)

EPOCHS : 29
Epoch 00030: reducing learning rate of group 0 to 5.0000e-04.

Test set: Average loss: 0.5647, Accuracy: 8509/10000 (85.09%)

EPOCHS : 30

Test set: Average loss: 0.4084, Accuracy: 8869/10000 (88.69%)

EPOCHS : 31

Test set: Average loss: 0.4028, Accuracy: 8920/10000 (89.20%)

EPOCHS : 32

Test set: Average loss: 0.3973, Accuracy: 8936/10000 (89.36%)

EPOCHS : 33

Test set: Average loss: 0.4046, Accuracy: 8923/10000 (89.23%)

EPOCHS : 34

Test set: Average loss: 0.4104, Accuracy: 8932/10000 (89.32%)

EPOCHS : 35

Test set: Average loss: 0.4078, Accuracy: 8942/10000 (89.42%)

EPOCHS : 36

Test set: Average loss: 0.4173, Accuracy: 8947/10000 (89.47%)

EPOCHS : 37

Test set: Average loss: 0.4187, Accuracy: 8959/10000 (89.59%)

EPOCHS : 38

Test set: Average loss: 0.4211, Accuracy: 8956/10000 (89.56%)

EPOCHS : 39

Test set: Average loss: 0.4216, Accuracy: 8954/10000 (89.54%)

Optimizer: RMSprop, Learning Rate: 0.01, Test Accuracy: 89.59
cuda
Files already downloaded and verified
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]          36,864
       BatchNorm2d-4           [-1, 64, 32, 32]             128
           Dropout-5           [-1, 64, 32, 32]               0
            Conv2d-6           [-1, 64, 32, 32]          36,864
       BatchNorm2d-7           [-1, 64, 32, 32]             128
           Dropout-8           [-1, 64, 32, 32]               0
        BasicBlock-9           [-1, 64, 32, 32]               0
           Conv2d-10           [-1, 64, 32, 32]          36,864
      BatchNorm2d-11           [-1, 64, 32, 32]             128
          Dropout-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 64, 32, 32]          36,864
      BatchNorm2d-14           [-1, 64, 32, 32]             128
          Dropout-15           [-1, 64, 32, 32]               0
       BasicBlock-16           [-1, 64, 32, 32]               0
           Conv2d-17          [-1, 128, 16, 16]          73,728
      BatchNorm2d-18          [-1, 128, 16, 16]             256
          Dropout-19          [-1, 128, 16, 16]               0
           Conv2d-20          [-1, 128, 16, 16]         147,456
      BatchNorm2d-21          [-1, 128, 16, 16]             256
          Dropout-22          [-1, 128, 16, 16]               0
           Conv2d-23          [-1, 128, 16, 16]           8,192
      BatchNorm2d-24          [-1, 128, 16, 16]             256
          Dropout-25          [-1, 128, 16, 16]               0
       BasicBlock-26          [-1, 128, 16, 16]               0
           Conv2d-27          [-1, 128, 16, 16]         147,456
      BatchNorm2d-28          [-1, 128, 16, 16]             256
          Dropout-29          [-1, 128, 16, 16]               0
           Conv2d-30          [-1, 128, 16, 16]         147,456
      BatchNorm2d-31          [-1, 128, 16, 16]             256
          Dropout-32          [-1, 128, 16, 16]               0
       BasicBlock-33          [-1, 128, 16, 16]               0
           Conv2d-34            [-1, 256, 8, 8]         294,912
      BatchNorm2d-35            [-1, 256, 8, 8]             512
          Dropout-36            [-1, 256, 8, 8]               0
           Conv2d-37            [-1, 256, 8, 8]         589,824
      BatchNorm2d-38            [-1, 256, 8, 8]             512
          Dropout-39            [-1, 256, 8, 8]               0
           Conv2d-40            [-1, 256, 8, 8]          32,768
      BatchNorm2d-41            [-1, 256, 8, 8]             512
          Dropout-42            [-1, 256, 8, 8]               0
       BasicBlock-43            [-1, 256, 8, 8]               0
           Conv2d-44            [-1, 256, 8, 8]         589,824
      BatchNorm2d-45            [-1, 256, 8, 8]             512
          Dropout-46            [-1, 256, 8, 8]               0
           Conv2d-47            [-1, 256, 8, 8]         589,824
      BatchNorm2d-48            [-1, 256, 8, 8]             512
          Dropout-49            [-1, 256, 8, 8]               0
       BasicBlock-50            [-1, 256, 8, 8]               0
           Conv2d-51            [-1, 512, 4, 4]       1,179,648
      BatchNorm2d-52            [-1, 512, 4, 4]           1,024
          Dropout-53            [-1, 512, 4, 4]               0
           Conv2d-54            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-55            [-1, 512, 4, 4]           1,024
          Dropout-56            [-1, 512, 4, 4]               0
           Conv2d-57            [-1, 512, 4, 4]         131,072
      BatchNorm2d-58            [-1, 512, 4, 4]           1,024
          Dropout-59            [-1, 512, 4, 4]               0
       BasicBlock-60            [-1, 512, 4, 4]               0
           Conv2d-61            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-62            [-1, 512, 4, 4]           1,024
          Dropout-63            [-1, 512, 4, 4]               0
           Conv2d-64            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-65            [-1, 512, 4, 4]           1,024
          Dropout-66            [-1, 512, 4, 4]               0
       BasicBlock-67            [-1, 512, 4, 4]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,173,962
Trainable params: 11,173,962
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 15.44
Params size (MB): 42.63
Estimated Total Size (MB): 58.07
----------------------------------------------------------------
Running with Optimizer: RMSprop, Learning Rate: 0.05
EPOCHS : 0

Test set: Average loss: 1.9160, Accuracy: 2867/10000 (28.67%)

EPOCHS : 1

Test set: Average loss: 1.7484, Accuracy: 3495/10000 (34.95%)

EPOCHS : 2

Test set: Average loss: 1.6634, Accuracy: 3886/10000 (38.86%)

EPOCHS : 3

Test set: Average loss: 1.5671, Accuracy: 4305/10000 (43.05%)

EPOCHS : 4

Test set: Average loss: 1.5756, Accuracy: 4392/10000 (43.92%)

EPOCHS : 5

Test set: Average loss: 1.4489, Accuracy: 4753/10000 (47.53%)

EPOCHS : 6

Test set: Average loss: 1.2803, Accuracy: 5313/10000 (53.13%)

EPOCHS : 7

Test set: Average loss: 1.1963, Accuracy: 5611/10000 (56.11%)

EPOCHS : 8

Test set: Average loss: 1.5349, Accuracy: 5271/10000 (52.71%)

EPOCHS : 9

Test set: Average loss: 1.1346, Accuracy: 6163/10000 (61.63%)

EPOCHS : 10

Test set: Average loss: 1.0008, Accuracy: 6642/10000 (66.42%)

EPOCHS : 11

Test set: Average loss: 0.9220, Accuracy: 6701/10000 (67.01%)

EPOCHS : 12

Test set: Average loss: 0.8049, Accuracy: 7350/10000 (73.50%)

EPOCHS : 13

Test set: Average loss: 0.9052, Accuracy: 7070/10000 (70.70%)

EPOCHS : 14

Test set: Average loss: 0.8119, Accuracy: 7319/10000 (73.19%)

EPOCHS : 15
Epoch 00016: reducing learning rate of group 0 to 2.5000e-03.

Test set: Average loss: 0.8375, Accuracy: 7321/10000 (73.21%)

EPOCHS : 16

Test set: Average loss: 0.5148, Accuracy: 8265/10000 (82.65%)

EPOCHS : 17

Test set: Average loss: 0.5074, Accuracy: 8280/10000 (82.80%)

EPOCHS : 18

Test set: Average loss: 0.4912, Accuracy: 8348/10000 (83.48%)

EPOCHS : 19

Test set: Average loss: 0.4825, Accuracy: 8382/10000 (83.82%)

EPOCHS : 20

Test set: Average loss: 0.4829, Accuracy: 8390/10000 (83.90%)

EPOCHS : 21

Test set: Average loss: 0.4813, Accuracy: 8397/10000 (83.97%)

EPOCHS : 22

Test set: Average loss: 0.4743, Accuracy: 8423/10000 (84.23%)

EPOCHS : 23
Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.

Test set: Average loss: 0.4710, Accuracy: 8448/10000 (84.48%)

EPOCHS : 24

Test set: Average loss: 0.4685, Accuracy: 8459/10000 (84.59%)

EPOCHS : 25

Test set: Average loss: 0.4681, Accuracy: 8461/10000 (84.61%)

EPOCHS : 26

Test set: Average loss: 0.4698, Accuracy: 8464/10000 (84.64%)

EPOCHS : 27

Test set: Average loss: 0.4688, Accuracy: 8462/10000 (84.62%)

EPOCHS : 28

Test set: Average loss: 0.4654, Accuracy: 8464/10000 (84.64%)

EPOCHS : 29
Epoch 00030: reducing learning rate of group 0 to 6.2500e-06.

Test set: Average loss: 0.4689, Accuracy: 8470/10000 (84.70%)

EPOCHS : 30

Test set: Average loss: 0.4705, Accuracy: 8461/10000 (84.61%)

EPOCHS : 31

Test set: Average loss: 0.4704, Accuracy: 8463/10000 (84.63%)

EPOCHS : 32
Epoch 00033: reducing learning rate of group 0 to 3.1250e-07.

Test set: Average loss: 0.4669, Accuracy: 8476/10000 (84.76%)

EPOCHS : 33

Test set: Average loss: 0.4712, Accuracy: 8458/10000 (84.58%)

EPOCHS : 34

Test set: Average loss: 0.4681, Accuracy: 8471/10000 (84.71%)

EPOCHS : 35
Epoch 00036: reducing learning rate of group 0 to 1.5625e-08.

Test set: Average loss: 0.4683, Accuracy: 8470/10000 (84.70%)

EPOCHS : 36

Test set: Average loss: 0.4637, Accuracy: 8487/10000 (84.87%)

EPOCHS : 37

Test set: Average loss: 0.4702, Accuracy: 8461/10000 (84.61%)

EPOCHS : 38

Test set: Average loss: 0.4661, Accuracy: 8475/10000 (84.75%)

EPOCHS : 39
Epoch 00040: reducing learning rate of group 0 to 7.8125e-10.

Test set: Average loss: 0.4716, Accuracy: 8457/10000 (84.57%)

Optimizer: RMSprop, Learning Rate: 0.05, Test Accuracy: 84.87
