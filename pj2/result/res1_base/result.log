cuda
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz
Extracting ./data/cifar-10-python.tar.gz to ./data
Files already downloaded and verified
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]          36,864
       BatchNorm2d-4           [-1, 64, 32, 32]             128
           Dropout-5           [-1, 64, 32, 32]               0
            Conv2d-6           [-1, 64, 32, 32]          36,864
       BatchNorm2d-7           [-1, 64, 32, 32]             128
           Dropout-8           [-1, 64, 32, 32]               0
        BasicBlock-9           [-1, 64, 32, 32]               0
           Conv2d-10           [-1, 64, 32, 32]          36,864
      BatchNorm2d-11           [-1, 64, 32, 32]             128
          Dropout-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 64, 32, 32]          36,864
      BatchNorm2d-14           [-1, 64, 32, 32]             128
          Dropout-15           [-1, 64, 32, 32]               0
       BasicBlock-16           [-1, 64, 32, 32]               0
           Conv2d-17          [-1, 128, 16, 16]          73,728
      BatchNorm2d-18          [-1, 128, 16, 16]             256
          Dropout-19          [-1, 128, 16, 16]               0
           Conv2d-20          [-1, 128, 16, 16]         147,456
      BatchNorm2d-21          [-1, 128, 16, 16]             256
          Dropout-22          [-1, 128, 16, 16]               0
           Conv2d-23          [-1, 128, 16, 16]           8,192
      BatchNorm2d-24          [-1, 128, 16, 16]             256
          Dropout-25          [-1, 128, 16, 16]               0
       BasicBlock-26          [-1, 128, 16, 16]               0
           Conv2d-27          [-1, 128, 16, 16]         147,456
      BatchNorm2d-28          [-1, 128, 16, 16]             256
          Dropout-29          [-1, 128, 16, 16]               0
           Conv2d-30          [-1, 128, 16, 16]         147,456
      BatchNorm2d-31          [-1, 128, 16, 16]             256
          Dropout-32          [-1, 128, 16, 16]               0
       BasicBlock-33          [-1, 128, 16, 16]               0
           Conv2d-34            [-1, 256, 8, 8]         294,912
      BatchNorm2d-35            [-1, 256, 8, 8]             512
          Dropout-36            [-1, 256, 8, 8]               0
           Conv2d-37            [-1, 256, 8, 8]         589,824
      BatchNorm2d-38            [-1, 256, 8, 8]             512
          Dropout-39            [-1, 256, 8, 8]               0
           Conv2d-40            [-1, 256, 8, 8]          32,768
      BatchNorm2d-41            [-1, 256, 8, 8]             512
          Dropout-42            [-1, 256, 8, 8]               0
       BasicBlock-43            [-1, 256, 8, 8]               0
           Conv2d-44            [-1, 256, 8, 8]         589,824
      BatchNorm2d-45            [-1, 256, 8, 8]             512
          Dropout-46            [-1, 256, 8, 8]               0
           Conv2d-47            [-1, 256, 8, 8]         589,824
      BatchNorm2d-48            [-1, 256, 8, 8]             512
          Dropout-49            [-1, 256, 8, 8]               0
       BasicBlock-50            [-1, 256, 8, 8]               0
           Conv2d-51            [-1, 512, 4, 4]       1,179,648
      BatchNorm2d-52            [-1, 512, 4, 4]           1,024
          Dropout-53            [-1, 512, 4, 4]               0
           Conv2d-54            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-55            [-1, 512, 4, 4]           1,024
          Dropout-56            [-1, 512, 4, 4]               0
           Conv2d-57            [-1, 512, 4, 4]         131,072
      BatchNorm2d-58            [-1, 512, 4, 4]           1,024
          Dropout-59            [-1, 512, 4, 4]               0
       BasicBlock-60            [-1, 512, 4, 4]               0
           Conv2d-61            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-62            [-1, 512, 4, 4]           1,024
          Dropout-63            [-1, 512, 4, 4]               0
           Conv2d-64            [-1, 512, 4, 4]       2,359,296
      BatchNorm2d-65            [-1, 512, 4, 4]           1,024
          Dropout-66            [-1, 512, 4, 4]               0
       BasicBlock-67            [-1, 512, 4, 4]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,173,962
Trainable params: 11,173,962
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 15.44
Params size (MB): 42.63
Estimated Total Size (MB): 58.07
----------------------------------------------------------------
EPOCHS : 0

Test set: Average loss: 1.3600, Accuracy: 5118/10000 (51.18%)

EPOCHS : 1

Test set: Average loss: 1.0902, Accuracy: 6189/10000 (61.89%)

EPOCHS : 2

Test set: Average loss: 1.0391, Accuracy: 6388/10000 (63.88%)

EPOCHS : 3

Test set: Average loss: 0.8410, Accuracy: 7100/10000 (71.00%)

EPOCHS : 4

Test set: Average loss: 0.6993, Accuracy: 7541/10000 (75.41%)

EPOCHS : 5

Test set: Average loss: 0.7863, Accuracy: 7462/10000 (74.62%)

EPOCHS : 6

Test set: Average loss: 0.7862, Accuracy: 7587/10000 (75.87%)

EPOCHS : 7

Test set: Average loss: 0.6030, Accuracy: 8084/10000 (80.84%)

EPOCHS : 8

Test set: Average loss: 0.7405, Accuracy: 7751/10000 (77.51%)

EPOCHS : 9

Test set: Average loss: 0.6442, Accuracy: 8052/10000 (80.52%)

EPOCHS : 10

Test set: Average loss: 0.8915, Accuracy: 7610/10000 (76.10%)

EPOCHS : 11

Test set: Average loss: 0.7298, Accuracy: 7983/10000 (79.83%)

EPOCHS : 12

Test set: Average loss: 0.6958, Accuracy: 8127/10000 (81.27%)

EPOCHS : 13

Test set: Average loss: 0.6365, Accuracy: 8285/10000 (82.85%)

EPOCHS : 14

Test set: Average loss: 0.7162, Accuracy: 8145/10000 (81.45%)

EPOCHS : 15

Test set: Average loss: 0.6930, Accuracy: 8241/10000 (82.41%)

EPOCHS : 16

Test set: Average loss: 0.6629, Accuracy: 8319/10000 (83.19%)

EPOCHS : 17

Test set: Average loss: 0.7490, Accuracy: 8210/10000 (82.10%)

EPOCHS : 18

Test set: Average loss: 0.5886, Accuracy: 8548/10000 (85.48%)

EPOCHS : 19

Test set: Average loss: 0.5828, Accuracy: 8591/10000 (85.91%)

EPOCHS : 20

Test set: Average loss: 0.5824, Accuracy: 8594/10000 (85.94%)

EPOCHS : 21

Test set: Average loss: 0.5815, Accuracy: 8591/10000 (85.91%)

EPOCHS : 22

Test set: Average loss: 0.5812, Accuracy: 8598/10000 (85.98%)

EPOCHS : 23

Test set: Average loss: 0.5822, Accuracy: 8595/10000 (85.95%)

EPOCHS : 24

Test set: Average loss: 0.5819, Accuracy: 8591/10000 (85.91%)

EPOCHS : 25

Test set: Average loss: 0.5803, Accuracy: 8607/10000 (86.07%)

EPOCHS : 26

Test set: Average loss: 0.5825, Accuracy: 8598/10000 (85.98%)

EPOCHS : 27

Test set: Average loss: 0.5822, Accuracy: 8602/10000 (86.02%)

EPOCHS : 28

Test set: Average loss: 0.5799, Accuracy: 8598/10000 (85.98%)

EPOCHS : 29

Test set: Average loss: 0.5827, Accuracy: 8606/10000 (86.06%)

EPOCHS : 30

Test set: Average loss: 0.5815, Accuracy: 8597/10000 (85.97%)

EPOCHS : 31

Test set: Average loss: 0.5815, Accuracy: 8601/10000 (86.01%)

EPOCHS : 32

Test set: Average loss: 0.5828, Accuracy: 8597/10000 (85.97%)

EPOCHS : 33

Test set: Average loss: 0.5820, Accuracy: 8604/10000 (86.04%)

EPOCHS : 34

Test set: Average loss: 0.5817, Accuracy: 8601/10000 (86.01%)

EPOCHS : 35

Test set: Average loss: 0.5820, Accuracy: 8600/10000 (86.00%)

EPOCHS : 36

Test set: Average loss: 0.5810, Accuracy: 8594/10000 (85.94%)

EPOCHS : 37

Test set: Average loss: 0.5815, Accuracy: 8605/10000 (86.05%)

EPOCHS : 38

Test set: Average loss: 0.5825, Accuracy: 8598/10000 (85.98%)

EPOCHS : 39

Test set: Average loss: 0.5821, Accuracy: 8597/10000 (85.97%)

